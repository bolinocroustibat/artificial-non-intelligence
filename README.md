# Artificial Non Intelligence


## Data

- As a SQLite database, located in /data.
- As a JSON file from Kaggle and generated by an external Notebook, located in /data.


## Database

SQL commands to create the database schema:

```sql
CREATE TABLE comments (
	id INTEGER NOT NULL PRIMARY KEY,
	content TEXT NOT NULL,
	real INTEGER NOT NULL,
	aggressive INTEGER,
	difficulty INTEGER,
	created timestamp DEFAULT CURRENT_TIMESTAMP
);
```

```sql
CREATE TABLE answers (
	id INTEGER NOT NULL PRIMARY KEY,
	answer INT NOT NULL,
	ip VARCHAR,
	COMMENT INTEGER NOT NULL,
	FOREIGN KEY (COMMENT) REFERENCES comments (id)
);
```


## Import data as JSON files into the database

Use the following script and edit it with the right filenames:
`python api/import_json.py`


## Remove duplicate from comments table

Use this SQL command after import:
```sql
DELETE FROM comments
WHERE id NOT in(
		SELECT
			min(id)
			FROM comments
		GROUP BY
			content);
```


## API

To launch the API locally:
`uvicorn api.main:app --reload`


## To download the local DB from Heroku

The database is updated with user's answers as it is online. To retrieve this information and avoid overwriting it when updating, one should download the database with the new information before any changes. To do that, use that command:
```sh
heroku ps:copy data/db.sqlite3 --app=non-intelligence-api --output=data/db.sqlite3
```

## To deploy on Heroku

Add Heroku app as origin, if necessary:
`heroku git:remote -a non-intelligence-api`

Deploy with:
`git push heroku main`


## Comments crawlers

Located in /scraper.

Python script to get comments from online website comments and put them in a SQlite database.

Launch the crawler with Python from typer command file:
```sh
python3 ./scraper/crawl.py [website-crawler]
```

for example, for the crawler "Le Figaro":
```sh
python3 ./scraper/crawl.py figaro
```
